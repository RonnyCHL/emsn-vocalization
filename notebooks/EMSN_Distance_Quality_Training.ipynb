{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMSN 2.0 - Distance & Quality CNN Training\n",
    "## Maximale kwaliteit classifier voor 29.000+ detecties\n",
    "\n",
    "### Wat dit model doet:\n",
    "- **Distance classificatie:** very_close (TUIN), close (NABIJ), medium (NABIJ), far (VER), very_far (VER)\n",
    "- **Quality classificatie:** excellent, good, fair, poor, very_poor\n",
    "- **Multi-task learning:** E√©n model voor beide taken\n",
    "\n",
    "### Data:\n",
    "- 29.419 gelabelde detecties uit EMSN PostgreSQL (gebalanceerd)\n",
    "- Labels gebaseerd op wiskundige analyse (RMS, SNR, spectral metrics)\n",
    "- Audio: 3-seconde MP3 fragmenten van BirdNET-Pi\n",
    "- **Data staat al op Google Drive!**\n",
    "\n",
    "### Colab Pro+ Instellingen:\n",
    "1. Runtime ‚Üí Change runtime type\n",
    "2. Hardware accelerator: **GPU**\n",
    "3. GPU type: **A100** (40GB)\n",
    "4. High-RAM: **‚úì Aan**\n",
    "\n",
    "### Verwachte tijd: ~1-2 uur met A100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 1: GPU & Environment Check ===\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "import psutil\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\nPyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "\n",
    "ram_gb = psutil.virtual_memory().total / 1e9\n",
    "print(f\"RAM: {ram_gb:.1f} GB\")\n",
    "if ram_gb < 20:\n",
    "    print(\"‚ö†Ô∏è Enable High-RAM in Runtime settings!\")\n",
    "else:\n",
    "    print(\"‚úÖ High RAM beschikbaar\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"GPU Memory: {gpu_mem:.1f} GB\")\n",
    "    \n",
    "    # Stability for A100\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    if 'A100' in gpu_name:\n",
    "        GPU_TYPE = 'A100'\n",
    "        BATCH_SIZE = 128  # A100 kan veel aan\n",
    "        print(f\"\\nüöÄ A100 gedetecteerd - Maximum performance mode\")\n",
    "    elif 'V100' in gpu_name:\n",
    "        GPU_TYPE = 'V100'\n",
    "        BATCH_SIZE = 64\n",
    "    else:\n",
    "        GPU_TYPE = 'T4'\n",
    "        BATCH_SIZE = 32\n",
    "else:\n",
    "    GPU_TYPE = 'CPU'\n",
    "    BATCH_SIZE = 16\n",
    "    print(\"‚ö†Ô∏è Geen GPU!\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\n‚úÖ Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 2: Mount Google Drive ===\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Check of training data aanwezig is\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "DRIVE_DATA_DIR = Path('/content/drive/MyDrive/EMSN/distance_quality_training')\n",
    "\n",
    "if DRIVE_DATA_DIR.exists():\n",
    "    csv_path = DRIVE_DATA_DIR / 'training_data.csv'\n",
    "    audio_dir = DRIVE_DATA_DIR / 'audio'\n",
    "    \n",
    "    if csv_path.exists():\n",
    "        print(f\"‚úÖ training_data.csv gevonden\")\n",
    "    else:\n",
    "        print(f\"‚ùå training_data.csv NIET gevonden\")\n",
    "    \n",
    "    if audio_dir.exists():\n",
    "        audio_count = len(list(audio_dir.glob('*.mp3')))\n",
    "        print(f\"‚úÖ Audio directory gevonden: {audio_count:,} MP3 files\")\n",
    "    else:\n",
    "        print(f\"‚ùå Audio directory NIET gevonden\")\n",
    "else:\n",
    "    print(f\"‚ùå Data directory niet gevonden: {DRIVE_DATA_DIR}\")\n",
    "    print(\"\\nZorg ervoor dat je prepare_distance_quality_training.py hebt gedraaid!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 3: Install Dependencies ===\n",
    "!pip install librosa scikit-learn scikit-image matplotlib tqdm audiomentations pandas numpy -q\n",
    "print(\"‚úÖ Dependencies ge√Ønstalleerd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 4: Configuration ===\n",
    "from pathlib import Path\n",
    "\n",
    "# Storage paths\n",
    "DRIVE_DATA_DIR = Path('/content/drive/MyDrive/EMSN/distance_quality_training')\n",
    "AUDIO_DIR = DRIVE_DATA_DIR / 'audio'\n",
    "CSV_PATH = DRIVE_DATA_DIR / 'training_data.csv'\n",
    "\n",
    "# Local working directories\n",
    "WORK_DIR = Path('/content/EMSN-Distance-Quality')\n",
    "MODELS_DIR = WORK_DIR / 'models'\n",
    "CACHE_DIR = WORK_DIR / 'cache'\n",
    "\n",
    "for d in [MODELS_DIR, CACHE_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Training Configuration - MAXIMAAL voor Pro+\n",
    "VERSION = '2026_ultimate'\n",
    "EPOCHS = 100  # Meer epochs, early stopping bepaalt wanneer te stoppen\n",
    "LEARNING_RATE = 0.001\n",
    "MIN_LR = 0.00001\n",
    "PATIENCE = 15  # Early stopping patience\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "# Data parameters\n",
    "SAMPLE_RATE = 48000\n",
    "SEGMENT_DURATION = 3.0\n",
    "N_MELS = 128\n",
    "N_FFT = 2048\n",
    "HOP_LENGTH = 512\n",
    "FMIN = 500\n",
    "FMAX = 12000  # Hoger voor vogels\n",
    "\n",
    "# Augmentation\n",
    "USE_AUGMENTATION = True\n",
    "AUGMENTATION_FACTOR = 3  # 3x data door augmentation\n",
    "\n",
    "# Classes\n",
    "DISTANCE_CLASSES = ['very_close', 'close', 'medium', 'far', 'very_far']\n",
    "QUALITY_CLASSES = ['excellent', 'good', 'fair', 'poor', 'very_poor']\n",
    "\n",
    "# Nederlandse labels voor display (zoals op Ulanzi)\n",
    "DISTANCE_NL = {\n",
    "    'very_close': 'TUIN',\n",
    "    'close': 'NABIJ',\n",
    "    'medium': 'NABIJ',\n",
    "    'far': 'VER',\n",
    "    'very_far': 'VER'\n",
    "}\n",
    "\n",
    "print(f\"üìä CONFIGURATIE:\")\n",
    "print(f\"   GPU: {GPU_TYPE}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Epochs: {EPOCHS} (met early stopping)\")\n",
    "print(f\"   Augmentation: {USE_AUGMENTATION} ({AUGMENTATION_FACTOR}x)\")\n",
    "print(f\"   Data: {DRIVE_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 5: Load Training Data from Google Drive ===\n",
    "import pandas as pd\n",
    "\n",
    "print(f\"üì• Loading training data from Google Drive...\")\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(f\"‚úÖ Loaded {len(df):,} records\")\n",
    "\n",
    "# Show distribution\n",
    "print(f\"\\nüìä Distance verdeling:\")\n",
    "print(df['distance_category'].value_counts())\n",
    "\n",
    "print(f\"\\nüìä Quality verdeling:\")\n",
    "print(df['quality_category'].value_counts())\n",
    "\n",
    "print(f\"\\nüìä Station verdeling:\")\n",
    "print(df['station'].value_counts())\n",
    "\n",
    "# Check audio files\n",
    "print(f\"\\nüîç Checking audio files...\")\n",
    "existing_files = 0\n",
    "missing_files = 0\n",
    "for audio_file in df['audio_file'].dropna():\n",
    "    if (AUDIO_DIR / audio_file).exists():\n",
    "        existing_files += 1\n",
    "    else:\n",
    "        missing_files += 1\n",
    "\n",
    "print(f\"   ‚úÖ Audio files aanwezig: {existing_files:,}\")\n",
    "print(f\"   ‚ùå Audio files ontbreken: {missing_files:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 6: Spectrogram Generation Functions ===\n",
    "import librosa\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "\n",
    "def load_audio(path, sr=SAMPLE_RATE):\n",
    "    \"\"\"Load and preprocess audio.\"\"\"\n",
    "    try:\n",
    "        audio, _ = librosa.load(str(path), sr=sr, mono=True)\n",
    "        return audio\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def augment_audio(audio, sr):\n",
    "    \"\"\"Apply audio augmentations.\"\"\"\n",
    "    augmented = [audio]  # Original\n",
    "    \n",
    "    if not USE_AUGMENTATION:\n",
    "        return augmented\n",
    "    \n",
    "    # Pitch shift\n",
    "    try:\n",
    "        augmented.append(librosa.effects.pitch_shift(audio, sr=sr, n_steps=2))\n",
    "        augmented.append(librosa.effects.pitch_shift(audio, sr=sr, n_steps=-2))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Time stretch\n",
    "    try:\n",
    "        stretched = librosa.effects.time_stretch(audio, rate=0.9)\n",
    "        if len(stretched) > len(audio):\n",
    "            stretched = stretched[:len(audio)]\n",
    "        else:\n",
    "            stretched = np.pad(stretched, (0, len(audio) - len(stretched)))\n",
    "        augmented.append(stretched)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Add noise\n",
    "    noise = np.random.normal(0, 0.005, len(audio))\n",
    "    augmented.append(audio + noise)\n",
    "    \n",
    "    # Volume changes\n",
    "    augmented.append(audio * 0.8)\n",
    "    augmented.append(audio * 1.2)\n",
    "    \n",
    "    return augmented[:AUGMENTATION_FACTOR + 1]\n",
    "\n",
    "def audio_to_spectrogram(audio, sr=SAMPLE_RATE):\n",
    "    \"\"\"Convert audio to mel spectrogram.\"\"\"\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio, sr=sr,\n",
    "        n_mels=N_MELS, n_fft=N_FFT, hop_length=HOP_LENGTH,\n",
    "        fmin=FMIN, fmax=FMAX\n",
    "    )\n",
    "    mel_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    \n",
    "    # Normalize to 0-1\n",
    "    mel_norm = (mel_db - mel_db.min()) / (mel_db.max() - mel_db.min() + 1e-8)\n",
    "    \n",
    "    # Resize to fixed shape\n",
    "    target_shape = (128, 128)\n",
    "    if mel_norm.shape != target_shape:\n",
    "        mel_norm = resize(mel_norm, target_shape, anti_aliasing=True)\n",
    "    \n",
    "    return mel_norm.astype(np.float32)\n",
    "\n",
    "def process_audio_file(args):\n",
    "    \"\"\"Process single audio file to spectrograms.\"\"\"\n",
    "    path, distance_label, quality_label = args\n",
    "    \n",
    "    audio = load_audio(path)\n",
    "    if audio is None:\n",
    "        return []\n",
    "    \n",
    "    results = []\n",
    "    augmented_audios = augment_audio(audio, SAMPLE_RATE)\n",
    "    \n",
    "    for aug_audio in augmented_audios:\n",
    "        spec = audio_to_spectrogram(aug_audio)\n",
    "        results.append((spec, distance_label, quality_label))\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úÖ Spectrogram functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 7: Multi-Task CNN Model ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DistanceQualityCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-task CNN for distance and quality classification.\n",
    "    Deep architecture optimized for A100.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_distance_classes=5, num_quality_classes=5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Shared feature extractor - 5 conv blocks\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1: 128x128 -> 64x64\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(0.1),\n",
    "            \n",
    "            # Block 2: 64x64 -> 32x32\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(0.1),\n",
    "            \n",
    "            # Block 3: 32x32 -> 16x16\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(0.2),\n",
    "            \n",
    "            # Block 4: 16x16 -> 8x8\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(0.2),\n",
    "            \n",
    "            # Block 5: 8x8 -> 4x4\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(0.3),\n",
    "        )\n",
    "        \n",
    "        # Shared dense layers\n",
    "        self.shared_fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512 * 4 * 4, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "        \n",
    "        # Distance head\n",
    "        self.distance_head = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_distance_classes)\n",
    "        )\n",
    "        \n",
    "        # Quality head\n",
    "        self.quality_head = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_quality_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        shared = self.shared_fc(features)\n",
    "        \n",
    "        distance_out = self.distance_head(shared)\n",
    "        quality_out = self.quality_head(shared)\n",
    "        \n",
    "        return distance_out, quality_out\n",
    "\n",
    "# Test model\n",
    "model = DistanceQualityCNN().to(device)\n",
    "dummy = torch.randn(1, 1, 128, 128).to(device)\n",
    "dist_out, qual_out = model(dummy)\n",
    "print(f\"‚úÖ Model created\")\n",
    "print(f\"   Distance output: {dist_out.shape}\")\n",
    "print(f\"   Quality output: {qual_out.shape}\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "del model, dummy\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 8: Training Dataset Class ===\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class DistanceQualityDataset(Dataset):\n",
    "    \"\"\"Dataset for distance/quality training.\"\"\"\n",
    "    \n",
    "    def __init__(self, spectrograms, distance_labels, quality_labels):\n",
    "        self.spectrograms = torch.FloatTensor(spectrograms).unsqueeze(1)\n",
    "        self.distance_labels = torch.LongTensor(distance_labels)\n",
    "        self.quality_labels = torch.LongTensor(quality_labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.spectrograms)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.spectrograms[idx],\n",
    "            self.distance_labels[idx],\n",
    "            self.quality_labels[idx]\n",
    "        )\n",
    "\n",
    "print(\"‚úÖ Dataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === CELL 9: Process Audio Files (PARALLEL) ===\nfrom tqdm.notebook import tqdm\nimport numpy as np\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\nimport multiprocessing as mp\n\n# Label encoding\ndistance_to_idx = {c: i for i, c in enumerate(DISTANCE_CLASSES)}\nquality_to_idx = {c: i for i, c in enumerate(QUALITY_CLASSES)}\n\nprint(f\"Distance classes: {distance_to_idx}\")\nprint(f\"Quality classes: {quality_to_idx}\")\n\n# Check if cached spectrograms exist\ncache_path = CACHE_DIR / 'preprocessed_spectrograms.npz'\n\nif cache_path.exists():\n    print(f\"\\nüìÇ Loading cached spectrograms...\")\n    data = np.load(cache_path)\n    X = data['spectrograms']\n    y_distance = data['distance_labels']\n    y_quality = data['quality_labels']\n    print(f\"‚úÖ Loaded {len(X):,} spectrograms from cache\")\nelse:\n    print(f\"\\nüîÑ Processing audio files from Google Drive (PARALLEL)...\")\n    \n    # Build list of files to process\n    valid_rows = []\n    for idx, row in df.iterrows():\n        if pd.isna(row['audio_file']):\n            continue\n        audio_path = AUDIO_DIR / row['audio_file']\n        if not audio_path.exists():\n            continue\n        dist_label = distance_to_idx.get(row['distance_category'])\n        qual_label = quality_to_idx.get(row['quality_category'])\n        if dist_label is not None and qual_label is not None:\n            valid_rows.append((audio_path, dist_label, qual_label))\n    \n    print(f\"Valid audio files: {len(valid_rows):,}\")\n    \n    # Parallel processing - use most CPUs\n    num_workers = min(mp.cpu_count() - 1, 8)  # Leave 1 CPU free, max 8\n    print(f\"Using {num_workers} parallel workers\\n\")\n    \n    all_spectrograms = []\n    all_distance_labels = []\n    all_quality_labels = []\n    \n    # Process in parallel with progress bar\n    with ProcessPoolExecutor(max_workers=num_workers) as executor:\n        # Submit all tasks\n        futures = {executor.submit(process_audio_file, args): args for args in valid_rows}\n        \n        # Collect results with progress bar\n        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing audio\"):\n            try:\n                results = future.result()\n                for spec, dist_label, qual_label in results:\n                    all_spectrograms.append(spec)\n                    all_distance_labels.append(dist_label)\n                    all_quality_labels.append(qual_label)\n            except Exception as e:\n                pass  # Skip failed files silently\n    \n    X = np.array(all_spectrograms)\n    y_distance = np.array(all_distance_labels)\n    y_quality = np.array(all_quality_labels)\n    \n    # Cache for next run\n    print(f\"\\nüíæ Saving to cache...\")\n    np.savez_compressed(cache_path, \n                        spectrograms=X, \n                        distance_labels=y_distance, \n                        quality_labels=y_quality)\n    \n    print(f\"\\n‚úÖ Processed {len(X):,} spectrograms (with augmentation)\")\n\nprint(f\"\\nüìä Dataset shape: {X.shape}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 10: Train/Val/Test Split ===\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split: train+val vs test (90% / 10%)\n",
    "X_trainval, X_test, y_dist_trainval, y_dist_test, y_qual_trainval, y_qual_test = train_test_split(\n",
    "    X, y_distance, y_quality, test_size=0.1, random_state=42, stratify=y_distance\n",
    ")\n",
    "\n",
    "# Second split: train vs val (80% / 20% of trainval = 72% / 18% of total)\n",
    "X_train, X_val, y_dist_train, y_dist_val, y_qual_train, y_qual_val = train_test_split(\n",
    "    X_trainval, y_dist_trainval, y_qual_trainval, test_size=0.2, random_state=42, stratify=y_dist_trainval\n",
    ")\n",
    "\n",
    "print(f\"üìä Data splits:\")\n",
    "print(f\"   Train: {len(X_train):,} ({100*len(X_train)/len(X):.0f}%)\")\n",
    "print(f\"   Val:   {len(X_val):,} ({100*len(X_val)/len(X):.0f}%)\")\n",
    "print(f\"   Test:  {len(X_test):,} ({100*len(X_test)/len(X):.0f}%)\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = DistanceQualityDataset(X_train, y_dist_train, y_qual_train)\n",
    "val_dataset = DistanceQualityDataset(X_val, y_dist_val, y_qual_val)\n",
    "test_dataset = DistanceQualityDataset(X_test, y_dist_test, y_qual_test)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=4, pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=4, pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=4, pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ DataLoaders created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 11: Training Loop ===\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Model, loss, optimizer\n",
    "model = DistanceQualityCNN(\n",
    "    num_distance_classes=len(DISTANCE_CLASSES),\n",
    "    num_quality_classes=len(QUALITY_CLASSES)\n",
    ").to(device)\n",
    "\n",
    "criterion_distance = nn.CrossEntropyLoss()\n",
    "criterion_quality = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "# Cosine annealing with warm restarts\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=MIN_LR)\n",
    "\n",
    "# Training tracking\n",
    "best_val_acc = 0\n",
    "best_model_state = None\n",
    "patience_counter = 0\n",
    "history = {'train_loss': [], 'val_loss': [], 'val_dist_acc': [], 'val_qual_acc': []}\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"üöÄ STARTING TRAINING\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Start: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "print(f\"Epochs: {EPOCHS} | Patience: {PATIENCE}\")\n",
    "print(f\"Batch size: {BATCH_SIZE} | LR: {LEARNING_RATE}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # === TRAINING ===\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for specs, dist_labels, qual_labels in train_loader:\n",
    "        specs = specs.to(device, non_blocking=True)\n",
    "        dist_labels = dist_labels.to(device, non_blocking=True)\n",
    "        qual_labels = qual_labels.to(device, non_blocking=True)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        dist_out, qual_out = model(specs)\n",
    "        \n",
    "        # Combined loss (weighted)\n",
    "        loss_dist = criterion_distance(dist_out, dist_labels)\n",
    "        loss_qual = criterion_quality(qual_out, qual_labels)\n",
    "        loss = 0.6 * loss_dist + 0.4 * loss_qual  # Distance is primary task\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    scheduler.step()\n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    # === VALIDATION ===\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    dist_correct = 0\n",
    "    qual_correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for specs, dist_labels, qual_labels in val_loader:\n",
    "            specs = specs.to(device, non_blocking=True)\n",
    "            dist_labels = dist_labels.to(device, non_blocking=True)\n",
    "            qual_labels = qual_labels.to(device, non_blocking=True)\n",
    "            \n",
    "            dist_out, qual_out = model(specs)\n",
    "            \n",
    "            loss_dist = criterion_distance(dist_out, dist_labels)\n",
    "            loss_qual = criterion_quality(qual_out, qual_labels)\n",
    "            loss = 0.6 * loss_dist + 0.4 * loss_qual\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            dist_correct += (dist_out.argmax(1) == dist_labels).sum().item()\n",
    "            qual_correct += (qual_out.argmax(1) == qual_labels).sum().item()\n",
    "            total += dist_labels.size(0)\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    val_dist_acc = dist_correct / total\n",
    "    val_qual_acc = qual_correct / total\n",
    "    combined_acc = (val_dist_acc + val_qual_acc) / 2\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_dist_acc'].append(val_dist_acc)\n",
    "    history['val_qual_acc'].append(val_qual_acc)\n",
    "    \n",
    "    # Check for improvement\n",
    "    if combined_acc > best_val_acc:\n",
    "        best_val_acc = combined_acc\n",
    "        best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "        patience_counter = 0\n",
    "        marker = '‚úÖ NEW BEST'\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        marker = ''\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:3d}/{EPOCHS} | \"\n",
    "          f\"Loss: {train_loss:.4f}/{val_loss:.4f} | \"\n",
    "          f\"Dist: {val_dist_acc:.1%} | Qual: {val_qual_acc:.1%} | \"\n",
    "          f\"LR: {lr:.6f} | {epoch_time:.0f}s {marker}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(f\"\\n‚èπÔ∏è Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üèÅ TRAINING COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Time: {total_time/60:.1f} minutes\")\n",
    "print(f\"Best combined accuracy: {best_val_acc:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 12: Evaluate on Test Set ===\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(best_model_state)\n",
    "model.eval()\n",
    "\n",
    "# Test evaluation\n",
    "all_dist_preds = []\n",
    "all_dist_true = []\n",
    "all_qual_preds = []\n",
    "all_qual_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for specs, dist_labels, qual_labels in test_loader:\n",
    "        specs = specs.to(device)\n",
    "        \n",
    "        dist_out, qual_out = model(specs)\n",
    "        \n",
    "        all_dist_preds.extend(dist_out.argmax(1).cpu().numpy())\n",
    "        all_dist_true.extend(dist_labels.numpy())\n",
    "        all_qual_preds.extend(qual_out.argmax(1).cpu().numpy())\n",
    "        all_qual_true.extend(qual_labels.numpy())\n",
    "\n",
    "# Distance report\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üìä DISTANCE CLASSIFICATION REPORT\")\n",
    "print(f\"{'='*60}\")\n",
    "print(classification_report(all_dist_true, all_dist_preds, target_names=DISTANCE_CLASSES))\n",
    "\n",
    "# Quality report\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üìä QUALITY CLASSIFICATION REPORT\")\n",
    "print(f\"{'='*60}\")\n",
    "print(classification_report(all_qual_true, all_qual_preds, target_names=QUALITY_CLASSES))\n",
    "\n",
    "# Confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Distance confusion matrix\n",
    "cm_dist = confusion_matrix(all_dist_true, all_dist_preds)\n",
    "sns.heatmap(cm_dist, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=DISTANCE_CLASSES, yticklabels=DISTANCE_CLASSES, ax=axes[0])\n",
    "axes[0].set_title('Distance Confusion Matrix')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('True')\n",
    "\n",
    "# Quality confusion matrix\n",
    "cm_qual = confusion_matrix(all_qual_true, all_qual_preds)\n",
    "sns.heatmap(cm_qual, annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=QUALITY_CLASSES, yticklabels=QUALITY_CLASSES, ax=axes[1])\n",
    "axes[1].set_title('Quality Confusion Matrix')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('True')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODELS_DIR / 'confusion_matrices.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 13: Training History Plot ===\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(history['train_loss'], label='Train Loss')\n",
    "axes[0].plot(history['val_loss'], label='Val Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training & Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(history['val_dist_acc'], label='Distance Acc')\n",
    "axes[1].plot(history['val_qual_acc'], label='Quality Acc')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODELS_DIR / 'training_history.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 14: Save Model ===\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "\n",
    "dist_acc = accuracy_score(all_dist_true, all_dist_preds)\n",
    "qual_acc = accuracy_score(all_qual_true, all_qual_preds)\n",
    "\n",
    "model_path = MODELS_DIR / f'distance_quality_cnn_{VERSION}.pt'\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': best_model_state,\n",
    "    'distance_classes': DISTANCE_CLASSES,\n",
    "    'quality_classes': QUALITY_CLASSES,\n",
    "    'distance_nl': DISTANCE_NL,\n",
    "    'distance_accuracy': dist_acc,\n",
    "    'quality_accuracy': qual_acc,\n",
    "    'version': VERSION,\n",
    "    'training_config': {\n",
    "        'epochs': len(history['train_loss']),\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'augmentation': USE_AUGMENTATION,\n",
    "        'n_mels': N_MELS,\n",
    "        'sample_rate': SAMPLE_RATE\n",
    "    },\n",
    "    'history': history\n",
    "}, model_path)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"‚úÖ MODEL SAVED\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Path: {model_path}\")\n",
    "print(f\"Size: {model_path.stat().st_size / 1e6:.1f} MB\")\n",
    "print(f\"Distance accuracy: {dist_acc:.1%}\")\n",
    "print(f\"Quality accuracy: {qual_acc:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 15: Save Model to Google Drive ===\n",
    "import shutil\n",
    "\n",
    "# Copy model to Google Drive for persistence\n",
    "drive_models_dir = DRIVE_DATA_DIR / 'models'\n",
    "drive_models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "drive_model_path = drive_models_dir / f'distance_quality_cnn_{VERSION}.pt'\n",
    "shutil.copy(model_path, drive_model_path)\n",
    "\n",
    "# Also copy plots\n",
    "shutil.copy(MODELS_DIR / 'confusion_matrices.png', drive_models_dir / 'confusion_matrices.png')\n",
    "shutil.copy(MODELS_DIR / 'training_history.png', drive_models_dir / 'training_history.png')\n",
    "\n",
    "print(f\"‚úÖ Model saved to Google Drive: {drive_model_path}\")\n",
    "print(f\"\\nNa training, download het model via:\")\n",
    "print(f\"   Drive ‚Üí EMSN ‚Üí distance_quality_training ‚Üí models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 16: Download Model (Optional) ===\n",
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "# Create ZIP with model and images\n",
    "print(\"üì¶ Creating download package...\")\n",
    "\n",
    "zip_name = f'emsn_distance_quality_{VERSION}'\n",
    "shutil.make_archive(f'/content/{zip_name}', 'zip', MODELS_DIR)\n",
    "\n",
    "zip_path = f'/content/{zip_name}.zip'\n",
    "zip_size = os.path.getsize(zip_path) / 1e6\n",
    "\n",
    "print(f\"\\n‚úÖ Package ready: {zip_path}\")\n",
    "print(f\"Size: {zip_size:.1f} MB\")\n",
    "\n",
    "print(\"\\nüì• Starting download...\")\n",
    "files.download(zip_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Na het downloaden\n",
    "\n",
    "Upload het model naar je Pi:\n",
    "```bash\n",
    "# Op je PC\n",
    "scp emsn_distance_quality_2026_ultimate.zip ronny@192.168.1.178:~/\n",
    "\n",
    "# Op de Pi\n",
    "unzip emsn_distance_quality_2026_ultimate.zip -d ~/emsn2/models/\n",
    "```\n",
    "\n",
    "Of download direct van Google Drive:\n",
    "```bash\n",
    "# Op de Pi met rclone\n",
    "rclone copy gdrive:/EMSN/distance_quality_training/models/ ~/emsn2/models/\n",
    "```\n",
    "\n",
    "Het model kan dan gebruikt worden door de quality_enricher voor real-time classificatie."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}