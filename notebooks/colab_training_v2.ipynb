{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# EMSN 2.0 - Vocalization Training v2\n## Verbeterde modellen met SpecAugment, Residual CNN & HiDrive sync\n\n### Verbeteringen t.o.v. v1 (ultimate):\n- **SpecAugment** - Frequency & time masking op spectrogrammen\n- **Meer augmentatie** - Volume scaling, achtergrondgeluid, pink noise\n- **Residual CNN** - Skip connections + SE attention\n- **Focal Loss** - Beter voor zeldzame klassen (alarm)\n- **Eigen data pipeline** - BirdNET-Pi detecties als extra trainingsdata\n- **Train/Val/Test split** - Apart test set voor eerlijke evaluatie\n- **Confusion matrix** - Per-soort analyse van type-verwarring\n- **Temperature scaling** - Gekalibreerde confidence scores\n- **HiDrive auto-upload** - Modellen direct naar Strato cloud (993 GB vrij)\n\n### Gebruik:\n1. Runtime > Change runtime type > **GPU** (A100 aanbevolen)\n2. High-RAM: **Aan**\n3. Plak je HiDrive SSH key in cel 3\n4. **Run All** - modellen worden automatisch ge-upload naar HiDrive\n5. Na afloop: download naar Pi met het commando in cel 12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cel 1: GPU & Systeem Check\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "import psutil\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\nPyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "\n",
    "ram_gb = psutil.virtual_memory().total / 1e9\n",
    "print(f\"RAM: {ram_gb:.1f} GB\")\n",
    "if ram_gb < 20:\n",
    "    print(\"\\u26a0\\ufe0f Low RAM! Enable High-RAM in Runtime settings\")\n",
    "else:\n",
    "    print(\"\\u2705 High RAM beschikbaar\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"GPU Memory: {gpu_mem:.1f} GB\")\n",
    "\n",
    "    # Stability settings\n",
    "    torch.backends.cuda.matmul.allow_tf32 = False\n",
    "    torch.backends.cudnn.allow_tf32 = False\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    if 'A100' in gpu_name:\n",
    "        GPU_TYPE = 'A100'\n",
    "        BATCH_SIZE = 64\n",
    "        print(f\"\\n\\U0001f680 A100 gedetecteerd - Maximum performance mode\")\n",
    "    elif 'V100' in gpu_name:\n",
    "        GPU_TYPE = 'V100'\n",
    "        BATCH_SIZE = 48\n",
    "    elif 'L4' in gpu_name:\n",
    "        GPU_TYPE = 'L4'\n",
    "        BATCH_SIZE = 48\n",
    "    else:\n",
    "        GPU_TYPE = 'T4'\n",
    "        BATCH_SIZE = 32\n",
    "else:\n",
    "    GPU_TYPE = 'CPU'\n",
    "    BATCH_SIZE = 16\n",
    "    print(\"\\u26a0\\ufe0f Geen GPU!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cel 2: Dependencies + rclone installeren\n!pip install librosa scikit-learn scikit-image matplotlib tqdm requests -q\n\n# rclone voor HiDrive sync\n!curl -s https://rclone.org/install.sh | bash -s beta 2>/dev/null || echo \"rclone al geinstalleerd\"\n!rclone version | head -1\n\nprint(\"\\u2705 Dependencies + rclone ge\\u00efnstalleerd\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cel 3: HiDrive SFTP verbinding + Opslag configuratie\n#\n# Stap 1: Plak je HiDrive SSH private key hieronder (id_ed25519_hidrive)\n# Stap 2: Draai deze cel - rclone wordt geconfigureerd en verbinding getest\n#\nimport os\nimport time\nimport json\nfrom pathlib import Path\nfrom datetime import datetime\n\n# === HIDRIVE SSH KEY ===\n# Plak hier de inhoud van ~/.ssh/id_ed25519_hidrive\nHIDRIVE_SSH_KEY = \"\"\"-----BEGIN OPENSSH PRIVATE KEY-----\nPLAK_HIER_JE_KEY\n-----END OPENSSH PRIVATE KEY-----\"\"\"\n\n# SSH key opslaan\nssh_dir = Path('/root/.ssh')\nssh_dir.mkdir(exist_ok=True)\nkey_path = ssh_dir / 'id_ed25519_hidrive'\nkey_path.write_text(HIDRIVE_SSH_KEY.strip() + '\\n')\nkey_path.chmod(0o600)\n\n# known_hosts toevoegen (voorkomt host key prompt)\n!ssh-keyscan sftp.hidrive.strato.com >> /root/.ssh/known_hosts 2>/dev/null\n\n# rclone configureren voor HiDrive SFTP\nrclone_config = \"\"\"[hidrive]\ntype = sftp\nhost = sftp.hidrive.strato.com\nuser = ronnyclouddisk\nkey_file = /root/.ssh/id_ed25519_hidrive\nshell_type = unix\n\"\"\"\nrclone_dir = Path('/root/.config/rclone')\nrclone_dir.mkdir(parents=True, exist_ok=True)\n(rclone_dir / 'rclone.conf').write_text(rclone_config)\n\n# HiDrive paden\nHIDRIVE_BASE = '/users/ronnyclouddisk/emsn-backups/vocalization-models'\nHIDRIVE_MODELS = f'{HIDRIVE_BASE}/v2'\nHIDRIVE_RESULTS = f'{HIDRIVE_BASE}/results'\n\n# Lokale paden\nDRIVE_BASE = '/content/EMSN-Vocalization'\nMODELS_DIR = f'{DRIVE_BASE}/models'\nAUDIO_DIR = f'{DRIVE_BASE}/audio'\nOWN_DATA_DIR = f'{DRIVE_BASE}/own_data'\n\nos.makedirs(MODELS_DIR, exist_ok=True)\nos.makedirs(AUDIO_DIR, exist_ok=True)\nos.makedirs(OWN_DATA_DIR, exist_ok=True)\n\n# Test verbinding en maak mappen aan\n!rclone mkdir hidrive:{HIDRIVE_MODELS}\n!rclone mkdir hidrive:{HIDRIVE_RESULTS}\n\n# Check ruimte\nprint(\"HiDrive verbinding testen...\")\nresult = !rclone about hidrive: 2>&1\nfor line in result:\n    print(f\"  {line}\")\n\n# Check bestaande modellen op HiDrive\nexisting = !rclone ls hidrive:{HIDRIVE_MODELS} 2>/dev/null | wc -l\nprint(f\"\\nBestaande v2 modellen op HiDrive: {existing[0].strip()}\")\n\n# === V2 CONFIGURATIE ===\nVERSION = '2025_v2'\n\n# Training parameters\nEPOCHS = 60\nLEARNING_RATE = 0.001\nMIN_LR = 0.00001\nPATIENCE = 12\nWEIGHT_DECAY = 0.01\n\n# Data parameters\nMAX_RECORDINGS_PER_TYPE = 50\nMAX_SEGMENTS_PER_RECORDING = 5\nNUM_WORKERS = 4\nMAX_CONCURRENT_DOWNLOADS = 10\n\n# Augmentation\nUSE_AUGMENTATION = True\nAUGMENTATION_FACTOR = 3\n\n# SpecAugment parameters\nFREQ_MASK_PARAM = 15\nTIME_MASK_PARAM = 20\nNUM_FREQ_MASKS = 2\nNUM_TIME_MASKS = 2\n\n# Focal Loss\nFOCAL_ALPHA = 0.25\nFOCAL_GAMMA = 2.0\n\n# Xeno-canto API key\nXC_API_KEY = '14258afd1c8a8e055387d012f2620e20f59ef3a2'\n\n# Upload functie voor gebruik na elke soort\ndef upload_model_to_hidrive(model_path):\n    \"\"\"Upload model naar HiDrive direct na training.\"\"\"\n    model_path = Path(model_path)\n    if model_path.exists():\n        os.system(f'rclone copy \"{model_path}\" hidrive:{HIDRIVE_MODELS}/ --progress')\n        return True\n    return False\n\ndef upload_results_to_hidrive():\n    \"\"\"Upload resultaten CSV en confusion JSON naar HiDrive.\"\"\"\n    for f in ['results_v2.csv', 'confusions_v2.json', 'results_v2.png', 'checkpoint_v2.csv']:\n        src = Path(f'{DRIVE_BASE}/{f}')\n        if src.exists():\n            os.system(f'rclone copy \"{src}\" hidrive:{HIDRIVE_RESULTS}/')\n\nprint(f\"\\n\\U0001f4ca EMSN VOCALIZATION TRAINING v2\")\nprint(f\"{'='*50}\")\nprint(f\"   GPU: {GPU_TYPE} | Batch: {BATCH_SIZE}\")\nprint(f\"   Epochs: {EPOCHS} | Patience: {PATIENCE}\")\nprint(f\"   Recordings/type: {MAX_RECORDINGS_PER_TYPE}\")\nprint(f\"   Augmentation: {AUGMENTATION_FACTOR}x + SpecAugment\")\nprint(f\"   Loss: Focal (gamma={FOCAL_GAMMA})\")\nprint(f\"   Architecture: Residual CNN\")\nprint(f\"   Opslag: HiDrive SFTP (auto-upload)\")\nprint(f\"   Version: {VERSION}\")\nprint(f\"{'='*50}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cel 4: Eigen data uploaden (optioneel)\n",
    "#\n",
    "# Upload eigen gelabelde audio bestanden vanuit BirdNET-Pi.\n",
    "# Structuur: own_data/{soortnaam}/{song|call|alarm}/*.mp3\n",
    "#\n",
    "# Voorbeeld:\n",
    "#   own_data/vink/song/Vink-95-2026-01-15-birdnet-08:30:00.mp3\n",
    "#   own_data/vink/call/Vink-88-2026-01-15-birdnet-09:15:00.mp3\n",
    "#\n",
    "# Tip: Exporteer geverifieerde detecties uit BirdNET-Pi en label ze.\n",
    "\n",
    "from google.colab import files as colab_files\n",
    "import zipfile\n",
    "\n",
    "UPLOAD_OWN_DATA = False  # Zet op True om eigen data te uploaden\n",
    "\n",
    "if UPLOAD_OWN_DATA:\n",
    "    print(\"\\U0001f4e4 Upload een ZIP met eigen gelabelde audio...\")\n",
    "    print(\"Verwachte structuur: {soortnaam}/{song|call|alarm}/*.mp3\")\n",
    "    print()\n",
    "    uploaded = colab_files.upload()\n",
    "\n",
    "    for filename, data in uploaded.items():\n",
    "        if filename.endswith('.zip'):\n",
    "            zip_path = f'/content/{filename}'\n",
    "            with open(zip_path, 'wb') as f:\n",
    "                f.write(data)\n",
    "            with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "                z.extractall(OWN_DATA_DIR)\n",
    "            print(f\"\\u2705 {filename} uitgepakt naar {OWN_DATA_DIR}\")\n",
    "\n",
    "    # Toon wat er ge-upload is\n",
    "    own_data_path = Path(OWN_DATA_DIR)\n",
    "    for species_dir in sorted(own_data_path.iterdir()):\n",
    "        if species_dir.is_dir():\n",
    "            for voc_dir in sorted(species_dir.iterdir()):\n",
    "                if voc_dir.is_dir():\n",
    "                    count = len(list(voc_dir.glob('*.mp3')))\n",
    "                    if count > 0:\n",
    "                        print(f\"  {species_dir.name}/{voc_dir.name}: {count} bestanden\")\n",
    "else:\n",
    "    print(\"\\u2139\\ufe0f Eigen data overgeslagen (UPLOAD_OWN_DATA = False)\")\n",
    "    print(\"Zet UPLOAD_OWN_DATA = True om BirdNET-Pi audio toe te voegen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cel 5: Alle 217 soorten\n",
    "\n",
    "ALL_SPECIES = [\n",
    "    # A\n",
    "    (\"Aalscholver\", \"Phalacrocorax carbo\", \"aalscholver\"),\n",
    "    (\"Appelvink\", \"Coccothraustes coccothraustes\", \"appelvink\"),\n",
    "    # B\n",
    "    (\"Baardman\", \"Panurus biarmicus\", \"baardman\"),\n",
    "    (\"Barmsijs\", \"Acanthis flammea\", \"barmsijs\"),\n",
    "    (\"Beflijster\", \"Turdus torquatus\", \"beflijster\"),\n",
    "    (\"Bergeend\", \"Tadorna tadorna\", \"bergeend\"),\n",
    "    (\"Bijeneter\", \"Merops apiaster\", \"bijeneter\"),\n",
    "    (\"Blauwborst\", \"Luscinia svecica\", \"blauwborst\"),\n",
    "    (\"Blauwe Kiekendief\", \"Circus cyaneus\", \"blauwe_kiekendief\"),\n",
    "    (\"Blauwe Reiger\", \"Ardea cinerea\", \"blauwe_reiger\"),\n",
    "    (\"Boerenzwaluw\", \"Hirundo rustica\", \"boerenzwaluw\"),\n",
    "    (\"Bokje\", \"Lymnocryptes minimus\", \"bokje\"),\n",
    "    (\"Bontbekplevier\", \"Charadrius hiaticula\", \"bontbekplevier\"),\n",
    "    (\"Bonte Kraai\", \"Corvus cornix\", \"bonte_kraai\"),\n",
    "    (\"Bonte Strandloper\", \"Calidris alpina\", \"bonte_strandloper\"),\n",
    "    (\"Bonte Vliegenvanger\", \"Ficedula hypoleuca\", \"bonte_vliegenvanger\"),\n",
    "    (\"Boomklever\", \"Sitta europaea\", \"boomklever\"),\n",
    "    (\"Boomkruiper\", \"Certhia brachydactyla\", \"boomkruiper\"),\n",
    "    (\"Boomleeuwerik\", \"Lullula arborea\", \"boomleeuwerik\"),\n",
    "    (\"Boompieper\", \"Anthus trivialis\", \"boompieper\"),\n",
    "    (\"Boomvalk\", \"Falco subbuteo\", \"boomvalk\"),\n",
    "    (\"Bosrietzanger\", \"Acrocephalus palustris\", \"bosrietzanger\"),\n",
    "    (\"Bosruiter\", \"Tringa glareola\", \"bosruiter\"),\n",
    "    (\"Bosuil\", \"Strix aluco\", \"bosuil\"),\n",
    "    (\"Braamsluiper\", \"Curruca curruca\", \"braamsluiper\"),\n",
    "    (\"Brandgans\", \"Branta leucopsis\", \"brandgans\"),\n",
    "    (\"Brilduiker\", \"Bucephala clangula\", \"brilduiker\"),\n",
    "    (\"Bruine Kiekendief\", \"Circus aeruginosus\", \"bruine_kiekendief\"),\n",
    "    (\"Buidelmees\", \"Remiz pendulinus\", \"buidelmees\"),\n",
    "    (\"Buizerd\", \"Buteo buteo\", \"buizerd\"),\n",
    "    # C\n",
    "    (\"Canadese Gans\", \"Branta canadensis\", \"canadese_gans\"),\n",
    "    (\"Cetti's Zanger\", \"Cettia cetti\", \"cettis_zanger\"),\n",
    "    (\"Citroenkanarie\", \"Crithagra citrinelloides\", \"citroenkanarie\"),\n",
    "    # D\n",
    "    (\"Dodaars\", \"Tachybaptus ruficollis\", \"dodaars\"),\n",
    "    (\"Draaihals\", \"Jynx torquilla\", \"draaihals\"),\n",
    "    (\"Drieteenstrandloper\", \"Calidris alba\", \"drieteenstrandloper\"),\n",
    "    (\"Dwergstern\", \"Sternula albifrons\", \"dwergstern\"),\n",
    "    # E\n",
    "    (\"Eider\", \"Somateria mollissima\", \"eider\"),\n",
    "    (\"Ekster\", \"Pica pica\", \"ekster\"),\n",
    "    (\"Europese Kanarie\", \"Serinus serinus\", \"europese_kanarie\"),\n",
    "    # F\n",
    "    (\"Fazant\", \"Phasianus colchicus\", \"fazant\"),\n",
    "    (\"Fitis\", \"Phylloscopus trochilus\", \"fitis\"),\n",
    "    (\"Flamingo\", \"Phoenicopterus roseus\", \"flamingo\"),\n",
    "    (\"Fluiter\", \"Phylloscopus sibilatrix\", \"fluiter\"),\n",
    "    (\"Fuut\", \"Podiceps cristatus\", \"fuut\"),\n",
    "    # G\n",
    "    (\"Gaai\", \"Garrulus glandarius\", \"gaai\"),\n",
    "    (\"Geelgors\", \"Emberiza citrinella\", \"geelgors\"),\n",
    "    (\"Gekraagde Roodstaart\", \"Phoenicurus phoenicurus\", \"gekraagde_roodstaart\"),\n",
    "    (\"Gele Kwikstaart\", \"Motacilla flava\", \"gele_kwikstaart\"),\n",
    "    (\"Gierzwaluw\", \"Apus apus\", \"gierzwaluw\"),\n",
    "    (\"Glanskop\", \"Poecile palustris\", \"glanskop\"),\n",
    "    (\"Goudhaan\", \"Regulus regulus\", \"goudhaan\"),\n",
    "    (\"Goudplevier\", \"Pluvialis apricaria\", \"goudplevier\"),\n",
    "    (\"Goudvink\", \"Pyrrhula pyrrhula\", \"goudvink\"),\n",
    "    (\"Grasmus\", \"Curruca communis\", \"grasmus\"),\n",
    "    (\"Graspieper\", \"Anthus pratensis\", \"graspieper\"),\n",
    "    (\"Graszanger\", \"Cisticola juncidis\", \"graszanger\"),\n",
    "    (\"Grauwe Gans\", \"Anser anser\", \"grauwe_gans\"),\n",
    "    (\"Grauwe Kiekendief\", \"Circus pygargus\", \"grauwe_kiekendief\"),\n",
    "    (\"Grauwe Klauwier\", \"Lanius collurio\", \"grauwe_klauwier\"),\n",
    "    (\"Grauwe Vliegenvanger\", \"Muscicapa striata\", \"grauwe_vliegenvanger\"),\n",
    "    (\"Groene Specht\", \"Picus viridis\", \"groene_specht\"),\n",
    "    (\"Groenling\", \"Chloris chloris\", \"groenling\"),\n",
    "    (\"Groenpootruiter\", \"Tringa nebularia\", \"groenpootruiter\"),\n",
    "    (\"Grote Bonte Specht\", \"Dendrocopos major\", \"grote_bonte_specht\"),\n",
    "    (\"Grote Canadese Gans\", \"Branta canadensis\", \"grote_canadese_gans\"),\n",
    "    (\"Grote Gele Kwikstaart\", \"Motacilla cinerea\", \"grote_gele_kwikstaart\"),\n",
    "    (\"Grote Karekiet\", \"Acrocephalus arundinaceus\", \"grote_karekiet\"),\n",
    "    (\"Grote Lijster\", \"Turdus viscivorus\", \"grote_lijster\"),\n",
    "    (\"Grote Mantelmeeuw\", \"Larus marinus\", \"grote_mantelmeeuw\"),\n",
    "    (\"Grote Zaagbek\", \"Mergus merganser\", \"grote_zaagbek\"),\n",
    "    (\"Grote Zilverreiger\", \"Ardea alba\", \"grote_zilverreiger\"),\n",
    "    (\"Grutto\", \"Limosa limosa\", \"grutto\"),\n",
    "    # H\n",
    "    (\"Haakbek\", \"Pinicola enucleator\", \"haakbek\"),\n",
    "    (\"Havik\", \"Accipiter gentilis\", \"havik\"),\n",
    "    (\"Heggenmus\", \"Prunella modularis\", \"heggenmus\"),\n",
    "    (\"Holenduif\", \"Columba oenas\", \"holenduif\"),\n",
    "    (\"Hop\", \"Upupa epops\", \"hop\"),\n",
    "    (\"Houtduif\", \"Columba palumbus\", \"houtduif\"),\n",
    "    (\"Houtsnip\", \"Scolopax rusticola\", \"houtsnip\"),\n",
    "    (\"Huismus\", \"Passer domesticus\", \"huismus\"),\n",
    "    (\"Huiszwaluw\", \"Delichon urbicum\", \"huiszwaluw\"),\n",
    "    # I\n",
    "    (\"IJsvogel\", \"Alcedo atthis\", \"ijsvogel\"),\n",
    "    # K\n",
    "    (\"Kanoetstrandloper\", \"Calidris canutus\", \"kanoetstrandloper\"),\n",
    "    (\"Kauw\", \"Coloeus monedula\", \"kauw\"),\n",
    "    (\"Keep\", \"Fringilla montifringilla\", \"keep\"),\n",
    "    (\"Kerkuil\", \"Tyto alba\", \"kerkuil\"),\n",
    "    (\"Kievit\", \"Vanellus vanellus\", \"kievit\"),\n",
    "    (\"Klapekster\", \"Lanius excubitor\", \"klapekster\"),\n",
    "    (\"Kleine Bonte Specht\", \"Dryobates minor\", \"kleine_bonte_specht\"),\n",
    "    (\"Kleine Karekiet\", \"Acrocephalus scirpaceus\", \"kleine_karekiet\"),\n",
    "    (\"Kleine Mantelmeeuw\", \"Larus fuscus\", \"kleine_mantelmeeuw\"),\n",
    "    (\"Kleine Rietgans\", \"Anser brachyrhynchus\", \"kleine_rietgans\"),\n",
    "    (\"Kleine Strandloper\", \"Calidris minuta\", \"kleine_strandloper\"),\n",
    "    (\"Kleine Zilverreiger\", \"Egretta garzetta\", \"kleine_zilverreiger\"),\n",
    "    (\"Kleine Zwaan\", \"Cygnus columbianus\", \"kleine_zwaan\"),\n",
    "    (\"Kluut\", \"Recurvirostra avosetta\", \"kluut\"),\n",
    "    (\"Kneu\", \"Linaria cannabina\", \"kneu\"),\n",
    "    (\"Knobbelzwaan\", \"Cygnus olor\", \"knobbelzwaan\"),\n",
    "    (\"Koekoek\", \"Cuculus canorus\", \"koekoek\"),\n",
    "    (\"Kokmeeuw\", \"Chroicocephalus ridibundus\", \"kokmeeuw\"),\n",
    "    (\"Kolgans\", \"Anser albifrons\", \"kolgans\"),\n",
    "    (\"Koolmees\", \"Parus major\", \"koolmees\"),\n",
    "    (\"Koperwiek\", \"Turdus iliacus\", \"koperwiek\"),\n",
    "    (\"Kraanvogel\", \"Grus grus\", \"kraanvogel\"),\n",
    "    (\"Krakeend\", \"Mareca strepera\", \"krakeend\"),\n",
    "    (\"Kramsvogel\", \"Turdus pilaris\", \"kramsvogel\"),\n",
    "    (\"Kruisbek\", \"Loxia curvirostra\", \"kruisbek\"),\n",
    "    (\"Kuifeend\", \"Aythya fuligula\", \"kuifeend\"),\n",
    "    (\"Kuifmees\", \"Lophophanes cristatus\", \"kuifmees\"),\n",
    "    (\"Kwak\", \"Nycticorax nycticorax\", \"kwak\"),\n",
    "    (\"Kwartel\", \"Coturnix coturnix\", \"kwartel\"),\n",
    "    (\"Kwartelkoning\", \"Crex crex\", \"kwartelkoning\"),\n",
    "    # M\n",
    "    (\"Mandarijneend\", \"Aix galericulata\", \"mandarijneend\"),\n",
    "    (\"Matkop\", \"Poecile montanus\", \"matkop\"),\n",
    "    (\"Meerkoet\", \"Fulica atra\", \"meerkoet\"),\n",
    "    (\"Merel\", \"Turdus merula\", \"merel\"),\n",
    "    (\"Middelste Zaagbek\", \"Mergus serrator\", \"middelste_zaagbek\"),\n",
    "    # N\n",
    "    (\"Nachtegaal\", \"Luscinia megarhynchos\", \"nachtegaal\"),\n",
    "    (\"Nachtzwaluw\", \"Caprimulgus europaeus\", \"nachtzwaluw\"),\n",
    "    (\"Nijlgans\", \"Alopochen aegyptiaca\", \"nijlgans\"),\n",
    "    (\"Nonnetje\", \"Mergellus albellus\", \"nonnetje\"),\n",
    "    # O\n",
    "    (\"Oehoe\", \"Bubo bubo\", \"oehoe\"),\n",
    "    (\"Oeverloper\", \"Actitis hypoleucos\", \"oeverloper\"),\n",
    "    (\"Oeverzwaluw\", \"Riparia riparia\", \"oeverzwaluw\"),\n",
    "    (\"Ooievaar\", \"Ciconia ciconia\", \"ooievaar\"),\n",
    "    # P\n",
    "    (\"Paapje\", \"Saxicola rubetra\", \"paapje\"),\n",
    "    (\"Patrijs\", \"Perdix perdix\", \"patrijs\"),\n",
    "    (\"Pestvogel\", \"Bombycilla garrulus\", \"pestvogel\"),\n",
    "    (\"Pijlstaart\", \"Anas acuta\", \"pijlstaart\"),\n",
    "    (\"Pimpelmees\", \"Cyanistes caeruleus\", \"pimpelmees\"),\n",
    "    (\"Porseleinhoen\", \"Porzana porzana\", \"porseleinhoen\"),\n",
    "    (\"Putter\", \"Carduelis carduelis\", \"putter\"),\n",
    "    # R\n",
    "    (\"Raaf\", \"Corvus corax\", \"raaf\"),\n",
    "    (\"Ransuil\", \"Asio otus\", \"ransuil\"),\n",
    "    (\"Regenwulp\", \"Numenius phaeopus\", \"regenwulp\"),\n",
    "    (\"Rietgors\", \"Emberiza schoeniclus\", \"rietgors\"),\n",
    "    (\"Rietzanger\", \"Acrocephalus schoenobaenus\", \"rietzanger\"),\n",
    "    (\"Rode Wouw\", \"Milvus milvus\", \"rode_wouw\"),\n",
    "    (\"Roek\", \"Corvus frugilegus\", \"roek\"),\n",
    "    (\"Roerdomp\", \"Botaurus stellaris\", \"roerdomp\"),\n",
    "    (\"Roodborst\", \"Erithacus rubecula\", \"roodborst\"),\n",
    "    (\"Roodborsttapuit\", \"Saxicola rubicola\", \"roodborsttapuit\"),\n",
    "    (\"Roodhalsfuut\", \"Podiceps grisegena\", \"roodhalsfuut\"),\n",
    "    (\"Rosse Grutto\", \"Limosa lapponica\", \"rosse_grutto\"),\n",
    "    (\"Rotsduif\", \"Columba livia\", \"rotsduif\"),\n",
    "    # S\n",
    "    (\"Scharrelaar\", \"Coracias garrulus\", \"scharrelaar\"),\n",
    "    (\"Scholekster\", \"Haematopus ostralegus\", \"scholekster\"),\n",
    "    (\"Sijs\", \"Spinus spinus\", \"sijs\"),\n",
    "    (\"Slechtvalk\", \"Falco peregrinus\", \"slechtvalk\"),\n",
    "    (\"Slobeend\", \"Spatula clypeata\", \"slobeend\"),\n",
    "    (\"Smelleken\", \"Falco columbarius\", \"smelleken\"),\n",
    "    (\"Smient\", \"Mareca penelope\", \"smient\"),\n",
    "    (\"Snor\", \"Locustella luscinioides\", \"snor\"),\n",
    "    (\"Sperwer\", \"Accipiter nisus\", \"sperwer\"),\n",
    "    (\"Spotvogel\", \"Hippolais icterina\", \"spotvogel\"),\n",
    "    (\"Spreeuw\", \"Sturnus vulgaris\", \"spreeuw\"),\n",
    "    (\"Sprinkhaanzanger\", \"Locustella naevia\", \"sprinkhaanzanger\"),\n",
    "    (\"Staartmees\", \"Aegithalos caudatus\", \"staartmees\"),\n",
    "    (\"Stadsduif\", \"Columba livia domestica\", \"stadsduif\"),\n",
    "    (\"Steenloper\", \"Arenaria interpres\", \"steenloper\"),\n",
    "    (\"Steenuil\", \"Athene noctua\", \"steenuil\"),\n",
    "    (\"Stormmeeuw\", \"Larus canus\", \"stormmeeuw\"),\n",
    "    # T\n",
    "    (\"Tafeleend\", \"Aythya ferina\", \"tafeleend\"),\n",
    "    (\"Taigaboomkruiper\", \"Certhia familiaris\", \"taigaboomkruiper\"),\n",
    "    (\"Tapuit\", \"Oenanthe oenanthe\", \"tapuit\"),\n",
    "    (\"Tjiftjaf\", \"Phylloscopus collybita\", \"tjiftjaf\"),\n",
    "    (\"Toendrarietgans\", \"Anser serrirostris\", \"toendrarietgans\"),\n",
    "    (\"Torenvalk\", \"Falco tinnunculus\", \"torenvalk\"),\n",
    "    (\"Tuinfluiter\", \"Sylvia borin\", \"tuinfluiter\"),\n",
    "    (\"Tureluur\", \"Tringa totanus\", \"tureluur\"),\n",
    "    (\"Turkse Tortel\", \"Streptopelia decaocto\", \"turkse_tortel\"),\n",
    "    # V\n",
    "    (\"Veldleeuwerik\", \"Alauda arvensis\", \"veldleeuwerik\"),\n",
    "    (\"Velduil\", \"Asio flammeus\", \"velduil\"),\n",
    "    (\"Vink\", \"Fringilla coelebs\", \"vink\"),\n",
    "    (\"Visdief\", \"Sterna hirundo\", \"visdief\"),\n",
    "    (\"Vuurgoudhaan\", \"Regulus ignicapilla\", \"vuurgoudhaan\"),\n",
    "    # W\n",
    "    (\"Waterhoen\", \"Gallinula chloropus\", \"waterhoen\"),\n",
    "    (\"Waterral\", \"Rallus aquaticus\", \"waterral\"),\n",
    "    (\"Watersnip\", \"Gallinago gallinago\", \"watersnip\"),\n",
    "    (\"Wielewaal\", \"Oriolus oriolus\", \"wielewaal\"),\n",
    "    (\"Wilde Eend\", \"Anas platyrhynchos\", \"wilde_eend\"),\n",
    "    (\"Wilde Zwaan\", \"Cygnus cygnus\", \"wilde_zwaan\"),\n",
    "    (\"Winterkoning\", \"Troglodytes troglodytes\", \"winterkoning\"),\n",
    "    (\"Wintertaling\", \"Anas crecca\", \"wintertaling\"),\n",
    "    (\"Witgat\", \"Tringa ochropus\", \"witgat\"),\n",
    "    (\"Witte Kwikstaart\", \"Motacilla alba\", \"witte_kwikstaart\"),\n",
    "    (\"Wulp\", \"Numenius arquata\", \"wulp\"),\n",
    "    # Z\n",
    "    (\"Zanglijster\", \"Turdus philomelos\", \"zanglijster\"),\n",
    "    (\"Zilvermeeuw\", \"Larus argentatus\", \"zilvermeeuw\"),\n",
    "    (\"Zomertortel\", \"Streptopelia turtur\", \"zomertortel\"),\n",
    "    (\"Zwarte Kraai\", \"Corvus corone\", \"zwarte_kraai\"),\n",
    "    (\"Zwarte Mees\", \"Periparus ater\", \"zwarte_mees\"),\n",
    "    (\"Zwarte Roodstaart\", \"Phoenicurus ochruros\", \"zwarte_roodstaart\"),\n",
    "    (\"Zwarte Ruiter\", \"Tringa erythropus\", \"zwarte_ruiter\"),\n",
    "    (\"Zwarte Specht\", \"Dryocopus martius\", \"zwarte_specht\"),\n",
    "    (\"Zwartkop\", \"Sylvia atricapilla\", \"zwartkop\"),\n",
    "]\n",
    "\n",
    "print(f\"Te trainen: {len(ALL_SPECIES)} soorten\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cel 6: Xeno-canto API + Download functies\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "def search_xeno_canto(scientific_name, voc_type='song', max_results=100):\n",
    "    \"\"\"Zoek opnames op Xeno-canto API v3.\"\"\"\n",
    "    parts = scientific_name.split()\n",
    "    if len(parts) < 2:\n",
    "        return []\n",
    "\n",
    "    genus, species = parts[0].lower(), parts[1].lower()\n",
    "\n",
    "    if ' ' in voc_type:\n",
    "        type_query = f'type:\"{voc_type}\"'\n",
    "    else:\n",
    "        type_query = f'type:{voc_type}'\n",
    "\n",
    "    # Kwaliteit A = beste opnames\n",
    "    query = f'gen:{genus} sp:{species} {type_query} q:A'\n",
    "    url = f'https://xeno-canto.org/api/3/recordings?query={query}&key={XC_API_KEY}'\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=30)\n",
    "        if response.status_code == 200:\n",
    "            return response.json().get('recordings', [])[:max_results]\n",
    "        return []\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "\n",
    "def download_single(args):\n",
    "    recording, output_dir = args\n",
    "    xc_id = recording['id']\n",
    "    file_url = recording.get('file', '')\n",
    "\n",
    "    if not file_url:\n",
    "        return None\n",
    "\n",
    "    if file_url.startswith('//'):\n",
    "        file_url = 'https:' + file_url\n",
    "    elif not file_url.startswith('http'):\n",
    "        file_url = 'https://xeno-canto.org' + file_url\n",
    "\n",
    "    output_path = output_dir / f\"XC{xc_id}.mp3\"\n",
    "\n",
    "    if output_path.exists():\n",
    "        return output_path\n",
    "\n",
    "    try:\n",
    "        response = requests.get(file_url, timeout=60)\n",
    "        if response.status_code == 200:\n",
    "            with open(output_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            return output_path\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "\n",
    "def download_recordings_parallel(recordings, output_dir, max_workers=10):\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    downloaded = []\n",
    "    args_list = [(rec, output_dir) for rec in recordings]\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {\n",
    "            executor.submit(download_single, args): args[0]['id']\n",
    "            for args in args_list\n",
    "        }\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                downloaded.append(result)\n",
    "\n",
    "    return downloaded\n",
    "\n",
    "\n",
    "print(\"\\u2705 Download functies geladen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cel 7: Spectrogram generatie + SpecAugment + Audio Augmentation\n",
    "import librosa\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial\n",
    "\n",
    "SAMPLE_RATE = 48000\n",
    "N_MELS = 128\n",
    "N_FFT = 2048\n",
    "HOP_LENGTH = 512\n",
    "FMIN = 500\n",
    "FMAX = 8000\n",
    "SEGMENT_DURATION = 3.0\n",
    "\n",
    "\n",
    "# --- SpecAugment: masking op spectrogrammen ---\n",
    "\n",
    "def spec_augment(mel_spec, num_freq_masks=NUM_FREQ_MASKS, freq_mask_param=FREQ_MASK_PARAM,\n",
    "                 num_time_masks=NUM_TIME_MASKS, time_mask_param=TIME_MASK_PARAM):\n",
    "    \"\"\"\n",
    "    SpecAugment: frequency en time masking op mel spectrogram.\n",
    "    Paper: Park et al. 2019 - \"SpecAugment: A Simple Data Augmentation Method\n",
    "    for Automatic Speech Recognition\"\n",
    "    \"\"\"\n",
    "    spec = mel_spec.copy()\n",
    "    n_mels, n_frames = spec.shape\n",
    "\n",
    "    # Frequency masking - maskeert horizontale banden\n",
    "    for _ in range(num_freq_masks):\n",
    "        f = np.random.randint(0, freq_mask_param + 1)\n",
    "        f0 = np.random.randint(0, max(1, n_mels - f))\n",
    "        spec[f0:f0 + f, :] = 0.0\n",
    "\n",
    "    # Time masking - maskeert verticale banden\n",
    "    for _ in range(num_time_masks):\n",
    "        t = np.random.randint(0, time_mask_param + 1)\n",
    "        t0 = np.random.randint(0, max(1, n_frames - t))\n",
    "        spec[:, t0:t0 + t] = 0.0\n",
    "\n",
    "    return spec\n",
    "\n",
    "\n",
    "# --- Audio augmentation functies ---\n",
    "\n",
    "def augment_audio(audio, sr):\n",
    "    \"\"\"\n",
    "    Genereer geaugmenteerde versies van audio.\n",
    "    Verbeterd t.o.v. v1: meer variatie, realistischer.\n",
    "    \"\"\"\n",
    "    augmented = []\n",
    "    target_len = len(audio)\n",
    "\n",
    "    # 1. Pitch shift (+/- 1-3 semitones, random)\n",
    "    try:\n",
    "        n_steps = np.random.choice([-3, -2, -1, 1, 2, 3])\n",
    "        shifted = librosa.effects.pitch_shift(audio, sr=sr, n_steps=n_steps)\n",
    "        augmented.append(shifted)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2. Time stretch (random factor 0.85-1.15)\n",
    "    try:\n",
    "        rate = np.random.uniform(0.85, 1.15)\n",
    "        stretched = librosa.effects.time_stretch(audio, rate=rate)\n",
    "        if len(stretched) > target_len:\n",
    "            stretched = stretched[:target_len]\n",
    "        else:\n",
    "            stretched = np.pad(stretched, (0, target_len - len(stretched)))\n",
    "        augmented.append(stretched)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 3. Volume scaling (random -6dB tot +6dB)\n",
    "    gain_db = np.random.uniform(-6, 6)\n",
    "    gain_linear = 10 ** (gain_db / 20)\n",
    "    volume_scaled = np.clip(audio * gain_linear, -1.0, 1.0)\n",
    "    augmented.append(volume_scaled)\n",
    "\n",
    "    # 4. Pink noise (realistischer dan Gaussian voor buitenopnames)\n",
    "    pink = _generate_pink_noise(target_len)\n",
    "    snr_db = np.random.uniform(15, 25)  # Realistische SNR range\n",
    "    signal_power = np.mean(audio ** 2) + 1e-10\n",
    "    noise_power = signal_power / (10 ** (snr_db / 10))\n",
    "    noisy = audio + pink * np.sqrt(noise_power / (np.mean(pink ** 2) + 1e-10))\n",
    "    augmented.append(np.clip(noisy, -1.0, 1.0).astype(np.float32))\n",
    "\n",
    "    # 5. Combinatie: pitch + noise\n",
    "    if len(augmented) >= 2:\n",
    "        combo = augmented[0] + pink * np.sqrt(noise_power / (np.mean(pink ** 2) + 1e-10)) * 0.5\n",
    "        augmented.append(np.clip(combo, -1.0, 1.0).astype(np.float32))\n",
    "\n",
    "    return augmented\n",
    "\n",
    "\n",
    "def _generate_pink_noise(n_samples):\n",
    "    \"\"\"Genereer pink noise (1/f) - realistischer dan wit/Gaussian.\"\"\"\n",
    "    white = np.random.randn(n_samples)\n",
    "    fft = np.fft.rfft(white)\n",
    "    freqs = np.fft.rfftfreq(n_samples)\n",
    "    freqs[0] = 1  # Voorkom deling door 0\n",
    "    fft = fft / np.sqrt(freqs)\n",
    "    pink = np.fft.irfft(fft, n=n_samples)\n",
    "    pink = pink / (np.max(np.abs(pink)) + 1e-10)\n",
    "    return pink.astype(np.float32)\n",
    "\n",
    "\n",
    "# --- Spectrogram conversie ---\n",
    "\n",
    "def audio_to_spectrogram(audio, sr=SAMPLE_RATE, apply_spec_augment=False):\n",
    "    \"\"\"Converteer audio naar mel spectrogram, optioneel met SpecAugment.\"\"\"\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio, sr=sr,\n",
    "        n_mels=N_MELS, n_fft=N_FFT, hop_length=HOP_LENGTH,\n",
    "        fmin=FMIN, fmax=FMAX\n",
    "    )\n",
    "    mel_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    mel_norm = (mel_db - mel_db.min()) / (mel_db.max() - mel_db.min() + 1e-8)\n",
    "\n",
    "    # SpecAugment toepassen op training data\n",
    "    if apply_spec_augment:\n",
    "        mel_norm = spec_augment(mel_norm)\n",
    "\n",
    "    if mel_norm.shape != (128, 128):\n",
    "        from skimage.transform import resize\n",
    "        mel_norm = resize(mel_norm, (128, 128), anti_aliasing=True)\n",
    "\n",
    "    return mel_norm.astype(np.float32)\n",
    "\n",
    "\n",
    "def process_single_audio(audio_path, max_segments=5, use_augmentation=True):\n",
    "    \"\"\"Verwerk audio naar spectrogrammen met augmentation + SpecAugment.\"\"\"\n",
    "    try:\n",
    "        audio, sr = librosa.load(str(audio_path), sr=SAMPLE_RATE, mono=True)\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "    segment_samples = int(SEGMENT_DURATION * SAMPLE_RATE)\n",
    "    spectrograms = []\n",
    "    segments_processed = 0\n",
    "\n",
    "    for i in range(0, len(audio), segment_samples):\n",
    "        if segments_processed >= max_segments:\n",
    "            break\n",
    "\n",
    "        segment = audio[i:i + segment_samples]\n",
    "        if len(segment) < segment_samples // 2:\n",
    "            continue\n",
    "\n",
    "        if len(segment) < segment_samples:\n",
    "            segment = np.pad(segment, (0, segment_samples - len(segment)))\n",
    "\n",
    "        # Origineel (zonder SpecAugment)\n",
    "        spec = audio_to_spectrogram(segment, apply_spec_augment=False)\n",
    "        spectrograms.append(spec)\n",
    "\n",
    "        # Origineel met SpecAugment\n",
    "        spec_aug = audio_to_spectrogram(segment, apply_spec_augment=True)\n",
    "        spectrograms.append(spec_aug)\n",
    "\n",
    "        if use_augmentation:\n",
    "            # Audio augmentaties\n",
    "            aug_segments = augment_audio(segment, SAMPLE_RATE)\n",
    "            for aug_seg in aug_segments[:AUGMENTATION_FACTOR]:\n",
    "                # Geaugmenteerde audio + SpecAugment (50% kans)\n",
    "                apply_sa = np.random.random() > 0.5\n",
    "                aug_spec = audio_to_spectrogram(aug_seg, apply_spec_augment=apply_sa)\n",
    "                spectrograms.append(aug_spec)\n",
    "\n",
    "        segments_processed += 1\n",
    "\n",
    "    return spectrograms\n",
    "\n",
    "\n",
    "def process_audio_files_parallel(audio_paths, max_segments=5, max_workers=4,\n",
    "                                  use_augmentation=True):\n",
    "    all_specs = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        func = partial(process_single_audio, max_segments=max_segments,\n",
    "                       use_augmentation=use_augmentation)\n",
    "        results = list(executor.map(func, audio_paths))\n",
    "    for specs in results:\n",
    "        all_specs.extend(specs)\n",
    "    return all_specs\n",
    "\n",
    "\n",
    "print(\"\\u2705 Spectrogram + SpecAugment + Audio Augmentation geladen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cel 8: Residual CNN Model + Focal Loss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual block met skip connection.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.dropout = nn.Dropout2d(0.2)\n",
    "\n",
    "        # Skip connection met 1x1 conv als dimensies niet matchen\n",
    "        self.skip = nn.Identity()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.skip = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.skip(x)\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.dropout(out)\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = F.relu(out + identity)  # Skip connection\n",
    "        return out\n",
    "\n",
    "\n",
    "class VocalizationResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual CNN voor vocalisatie classificatie.\n",
    "    Verbeteringen t.o.v. v1 (ultimate):\n",
    "    - Skip connections voor betere gradient flow\n",
    "    - Channel attention (SE block) op bottleneck\n",
    "    - Global average pooling ipv flatten\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # Stem: initieel conv blok\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=7, stride=2, padding=3),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        # Residual blocks: 32 -> 64 -> 128 -> 256\n",
    "        self.layer1 = ResidualBlock(32, 64, stride=1)\n",
    "        self.layer2 = ResidualBlock(64, 128, stride=2)\n",
    "        self.layer3 = ResidualBlock(128, 256, stride=2)\n",
    "\n",
    "        # Squeeze-and-Excitation (channel attention)\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 256),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Global Average Pooling + Classifier\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        # Channel attention\n",
    "        se_weight = self.se(x).unsqueeze(-1).unsqueeze(-1)\n",
    "        x = x * se_weight\n",
    "\n",
    "        x = self.gap(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal Loss voor ongebalanceerde klassen.\n",
    "    Geeft meer gewicht aan moeilijke/zeldzame voorbeelden (alarm).\n",
    "    Paper: Lin et al. 2017 - \"Focal Loss for Dense Object Detection\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha=None, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha  # Per-class weging\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, weight=self.alpha, reduction='none')\n",
    "        p_t = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - p_t) ** self.gamma) * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\u2705 Residual CNN + Focal Loss klaar voor {device}\")\n",
    "\n",
    "# Test model\n",
    "test_model = VocalizationResNet(num_classes=3).to(device)\n",
    "test_input = torch.randn(4, 1, 128, 128).to(device)\n",
    "test_output = test_model(test_input)\n",
    "params = sum(p.numel() for p in test_model.parameters())\n",
    "print(f\"   Parameters: {params:,} ({params/1e6:.1f}M)\")\n",
    "print(f\"   Output shape: {test_output.shape}\")\n",
    "del test_model, test_input, test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cel 9: Training Pipeline v2 met alle verbeteringen\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def compute_class_weights(y, num_classes, device):\n",
    "    \"\"\"Bereken inverse frequency weights voor Focal Loss.\"\"\"\n",
    "    counts = np.bincount(y, minlength=num_classes).astype(np.float32)\n",
    "    counts = np.maximum(counts, 1)  # Voorkom deling door 0\n",
    "    weights = 1.0 / counts\n",
    "    weights = weights / weights.sum() * num_classes  # Normaliseer\n",
    "    return torch.FloatTensor(weights).to(device)\n",
    "\n",
    "\n",
    "def calibrate_temperature(model, val_loader, device):\n",
    "    \"\"\"\n",
    "    Temperature scaling voor gekalibreerde confidence.\n",
    "    Zoekt optimale temperature T zodat softmax(logits/T) goed gekalibreerd is.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    logits_list, labels_list = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            logits = model(X_batch)\n",
    "            logits_list.append(logits.cpu())\n",
    "            labels_list.append(y_batch)\n",
    "\n",
    "    logits_all = torch.cat(logits_list)\n",
    "    labels_all = torch.cat(labels_list)\n",
    "\n",
    "    # Grid search voor optimale temperature\n",
    "    best_t = 1.0\n",
    "    best_nll = float('inf')\n",
    "\n",
    "    for t in np.arange(0.5, 5.0, 0.1):\n",
    "        scaled = logits_all / t\n",
    "        nll = F.cross_entropy(scaled, labels_all).item()\n",
    "        if nll < best_nll:\n",
    "            best_nll = nll\n",
    "            best_t = t\n",
    "\n",
    "    return best_t\n",
    "\n",
    "\n",
    "def train_species_v2(dutch_name, scientific_name, dirname):\n",
    "    \"\"\"\n",
    "    V2 training met SpecAugment, Residual CNN, Focal Loss,\n",
    "    eigen data, train/val/test split en temperature scaling.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"\\U0001f426 {dutch_name} ({scientific_name})\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    audio_dir = Path(f'{DRIVE_BASE}/audio/{dirname}')\n",
    "\n",
    "    X_all, y_all = [], []\n",
    "    voc_types = [('song', 0), ('call', 1), ('alarm call', 2)]\n",
    "\n",
    "    # --- Stap 1: Download Xeno-canto data ---\n",
    "    for voc_type, label in voc_types:\n",
    "        print(f\"  \\U0001f4e5 {voc_type}...\", end=' ')\n",
    "        recordings = search_xeno_canto(scientific_name, voc_type,\n",
    "                                       max_results=MAX_RECORDINGS_PER_TYPE)\n",
    "\n",
    "        if not recordings:\n",
    "            print(\"0 gevonden\")\n",
    "            continue\n",
    "\n",
    "        type_dir = audio_dir / voc_type.replace(' ', '_')\n",
    "        audio_files = download_recordings_parallel(\n",
    "            recordings[:MAX_RECORDINGS_PER_TYPE],\n",
    "            type_dir,\n",
    "            max_workers=MAX_CONCURRENT_DOWNLOADS\n",
    "        )\n",
    "        print(f\"{len(audio_files)} files\", end=' ')\n",
    "\n",
    "        if audio_files:\n",
    "            specs = process_audio_files_parallel(\n",
    "                audio_files,\n",
    "                max_segments=MAX_SEGMENTS_PER_RECORDING,\n",
    "                max_workers=NUM_WORKERS,\n",
    "                use_augmentation=USE_AUGMENTATION\n",
    "            )\n",
    "            for spec in specs:\n",
    "                X_all.append(spec)\n",
    "                y_all.append(label)\n",
    "            print(f\"\\u2192 {len(specs)} specs\")\n",
    "        else:\n",
    "            print()\n",
    "\n",
    "    # --- Stap 2: Eigen data toevoegen ---\n",
    "    own_species_dir = Path(OWN_DATA_DIR) / dirname\n",
    "    if own_species_dir.exists():\n",
    "        own_count = 0\n",
    "        type_map = {'song': 0, 'call': 1, 'alarm': 2}\n",
    "        for voc_name, label in type_map.items():\n",
    "            own_type_dir = own_species_dir / voc_name\n",
    "            if own_type_dir.exists():\n",
    "                own_files = list(own_type_dir.glob('*.mp3'))\n",
    "                if own_files:\n",
    "                    # Eigen data: meer augmentatie (5x) want het is geverifieerd\n",
    "                    specs = process_audio_files_parallel(\n",
    "                        own_files,\n",
    "                        max_segments=MAX_SEGMENTS_PER_RECORDING,\n",
    "                        max_workers=NUM_WORKERS,\n",
    "                        use_augmentation=True\n",
    "                    )\n",
    "                    for spec in specs:\n",
    "                        X_all.append(spec)\n",
    "                        y_all.append(label)\n",
    "                    own_count += len(specs)\n",
    "        if own_count > 0:\n",
    "            print(f\"  \\U0001f3af Eigen data: +{own_count} specs\")\n",
    "\n",
    "    if len(X_all) < 30:\n",
    "        print(f\"  \\u26a0\\ufe0f Te weinig data ({len(X_all)})\")\n",
    "        return None, 'insufficient_data', {}\n",
    "\n",
    "    X = np.array(X_all)\n",
    "    y = np.array(y_all)\n",
    "\n",
    "    # Label remapping\n",
    "    unique_labels = np.unique(y)\n",
    "    num_classes = len(unique_labels)\n",
    "\n",
    "    if num_classes < 2:\n",
    "        print(f\"  \\u26a0\\ufe0f Slechts 1 klasse\")\n",
    "        return None, 'single_class', {}\n",
    "\n",
    "    label_map = {old: new for new, old in enumerate(unique_labels)}\n",
    "    y_remapped = np.array([label_map[l] for l in y])\n",
    "\n",
    "    all_class_names = ['song', 'call', 'alarm']\n",
    "    class_names = [all_class_names[l] for l in unique_labels]\n",
    "\n",
    "    unique, counts = np.unique(y_remapped, return_counts=True)\n",
    "    class_dist = {class_names[i]: int(counts[i]) for i in range(len(counts))}\n",
    "    print(f\"  \\U0001f4ca {len(X)} specs: {class_dist}\")\n",
    "\n",
    "    # --- Stap 3: Train/Val/Test split (70/15/15) ---\n",
    "    X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "        X, y_remapped, test_size=0.15, random_state=42, stratify=y_remapped\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_trainval, y_trainval, test_size=0.176,  # 0.176 * 0.85 ~= 0.15\n",
    "        random_state=42, stratify=y_trainval\n",
    "    )\n",
    "    print(f\"  Split: train={len(X_train)} val={len(X_val)} test={len(X_test)}\")\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        TensorDataset(torch.FloatTensor(X_train).unsqueeze(1),\n",
    "                      torch.LongTensor(y_train)),\n",
    "        batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        TensorDataset(torch.FloatTensor(X_val).unsqueeze(1),\n",
    "                      torch.LongTensor(y_val)),\n",
    "        batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        TensorDataset(torch.FloatTensor(X_test).unsqueeze(1),\n",
    "                      torch.LongTensor(y_test)),\n",
    "        batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=True\n",
    "    )\n",
    "\n",
    "    # --- Stap 4: Model + Focal Loss + Optimizer ---\n",
    "    model = VocalizationResNet(num_classes=num_classes).to(device)\n",
    "    class_weights = compute_class_weights(y_train, num_classes, device)\n",
    "    criterion = FocalLoss(alpha=class_weights, gamma=FOCAL_GAMMA)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE,\n",
    "                                  weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2,\n",
    "                                            eta_min=MIN_LR)\n",
    "\n",
    "    # --- Stap 5: Training loop ---\n",
    "    best_acc = 0\n",
    "    best_state = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    try:\n",
    "        for epoch in range(EPOCHS):\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch = X_batch.to(device, non_blocking=True)\n",
    "                y_batch = y_batch.to(device, non_blocking=True)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            scheduler.step(epoch)\n",
    "\n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_correct = 0\n",
    "            with torch.no_grad():\n",
    "                for X_batch, y_batch in val_loader:\n",
    "                    X_batch = X_batch.to(device, non_blocking=True)\n",
    "                    y_batch = y_batch.to(device, non_blocking=True)\n",
    "                    outputs = model(X_batch)\n",
    "                    val_correct += (outputs.argmax(1) == y_batch).sum().item()\n",
    "\n",
    "            val_acc = val_correct / len(y_val)\n",
    "\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"  \\u23f9\\ufe0f Early stop @ epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        if 'CUDA' in str(e):\n",
    "            print(f\"  \\u26a0\\ufe0f CUDA error\")\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            if best_state is None:\n",
    "                return None, 'cuda_error', {}\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    if best_state is None:\n",
    "        return None, 'training_failed', {}\n",
    "\n",
    "    # --- Stap 6: Test evaluatie ---\n",
    "    model.load_state_dict(best_state)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    test_preds, test_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            test_preds.extend(outputs.argmax(1).cpu().numpy())\n",
    "            test_labels.extend(y_batch.numpy())\n",
    "\n",
    "    test_acc = np.mean(np.array(test_preds) == np.array(test_labels))\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(test_labels, test_preds, labels=list(range(num_classes)))\n",
    "    cm_dict = {}\n",
    "    for i, true_name in enumerate(class_names):\n",
    "        for j, pred_name in enumerate(class_names):\n",
    "            if cm[i, j] > 0:\n",
    "                cm_dict[f\"{true_name}>{pred_name}\"] = int(cm[i, j])\n",
    "\n",
    "    # --- Stap 7: Temperature scaling ---\n",
    "    temperature = calibrate_temperature(model, val_loader, device)\n",
    "\n",
    "    # --- Stap 8: Model opslaan ---\n",
    "    model_path = Path(f'{DRIVE_BASE}/models/{dirname}_cnn_{VERSION}.pt')\n",
    "    torch.save({\n",
    "        'model_state_dict': best_state,\n",
    "        'num_classes': num_classes,\n",
    "        'class_names': class_names,\n",
    "        'label_map': label_map,\n",
    "        'accuracy': best_acc,\n",
    "        'test_accuracy': float(test_acc),\n",
    "        'temperature': temperature,\n",
    "        'confusion_matrix': cm_dict,\n",
    "        'class_distribution': class_dist,\n",
    "        'species_name': dutch_name,\n",
    "        'scientific_name': scientific_name,\n",
    "        'version': VERSION,\n",
    "        'architecture': 'ResNet_SE',\n",
    "    }, model_path)\n",
    "\n",
    "    del model, train_loader, val_loader, test_loader\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"  \\u2705 {model_path.name} | Val: {best_acc:.1%} | Test: {test_acc:.1%} | T={temperature:.2f} | {elapsed:.0f}s\")\n",
    "\n",
    "    if cm_dict:\n",
    "        # Toon alleen verwarringen (true != pred)\n",
    "        confusions = {k: v for k, v in cm_dict.items() if k.split('>')[0] != k.split('>')[1]}\n",
    "        if confusions:\n",
    "            print(f\"  \\U0001f500 Verwarringen: {confusions}\")\n",
    "\n",
    "    return test_acc, 'success', cm_dict\n",
    "\n",
    "\n",
    "print(\"\\u2705 V2 Training pipeline geladen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cel 10: Start Training (met auto-upload naar HiDrive)\nimport pandas as pd\n\nresults = []\nall_confusions = {}\nstart_all = time.time()\n\nprint(f\"{'='*60}\")\nprint(f\"\\U0001f680 EMSN VOCALIZATION TRAINING v2\")\nprint(f\"{'='*60}\")\nprint(f\"Start: {datetime.now().strftime('%H:%M:%S')}\")\nprint(f\"Soorten: {len(ALL_SPECIES)}\")\nprint(f\"GPU: {GPU_TYPE} | Architecture: ResNet+SE\")\nprint(f\"Augmentation: {AUGMENTATION_FACTOR}x + SpecAugment\")\nprint(f\"Loss: Focal (gamma={FOCAL_GAMMA})\")\nprint(f\"Opslag: HiDrive (auto-upload per model)\")\nprint(f\"{'='*60}\")\n\nsuccessful = 0\nfailed = 0\n\nfor i, (dutch, scientific, dirname) in enumerate(ALL_SPECIES):\n    try:\n        acc, status, cm = train_species_v2(dutch, scientific, dirname)\n        results.append({\n            'species': dutch,\n            'scientific': scientific,\n            'test_accuracy': acc,\n            'status': status\n        })\n        if cm:\n            all_confusions[dutch] = cm\n\n        if status == 'success':\n            successful += 1\n            # Direct uploaden naar HiDrive na succesvolle training\n            model_file = f'{DRIVE_BASE}/models/{dirname}_cnn_{VERSION}.pt'\n            upload_model_to_hidrive(model_file)\n        else:\n            failed += 1\n\n    except Exception as e:\n        print(f\"  \\u274c Error: {str(e)[:80]}\")\n        results.append({\n            'species': dutch,\n            'scientific': scientific,\n            'test_accuracy': None,\n            'status': 'error'\n        })\n        failed += 1\n\n    # Checkpoint elke 20 soorten: resultaten opslaan + uploaden\n    if (i + 1) % 20 == 0:\n        pd.DataFrame(results).to_csv(f'{DRIVE_BASE}/checkpoint_v2.csv', index=False)\n        with open(f'{DRIVE_BASE}/confusions_v2.json', 'w') as f:\n            json.dump(all_confusions, f, indent=2)\n        upload_results_to_hidrive()\n        elapsed = time.time() - start_all\n        eta = (elapsed / (i + 1)) * (len(ALL_SPECIES) - i - 1)\n        print(f\"\\n  \\U0001f4be [{i+1}/{len(ALL_SPECIES)}] \\u2705{successful} \\u274c{failed} | HiDrive synced | ETA: {eta/60:.0f}min\\n\")\n\n# Finale resultaten uploaden\nelapsed_all = time.time() - start_all\nprint(f\"\\n{'='*60}\")\nprint(f\"\\U0001f3c1 TRAINING VOLTOOID!\")\nprint(f\"{'='*60}\")\nprint(f\"Tijd: {elapsed_all/60:.1f} minuten\")\nprint(f\"Succesvol: {successful}/{len(ALL_SPECIES)}\")\nprint(f\"Mislukt: {failed}/{len(ALL_SPECIES)}\")\nprint(f\"\\nFinale upload naar HiDrive...\")\nupload_results_to_hidrive()\nprint(f\"\\u2705 Alle modellen staan op HiDrive!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cel 11: Resultaten & Analyse\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(f'{DRIVE_BASE}/results_v2.csv', index=False)\n",
    "\n",
    "# Confusion data opslaan\n",
    "with open(f'{DRIVE_BASE}/confusions_v2.json', 'w') as f:\n",
    "    json.dump(all_confusions, f, indent=2)\n",
    "\n",
    "successful_df = df[df['status'] == 'success']\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"\\U0001f4ca RESULTATEN v2\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Getraind: {len(successful_df)}/{len(df)}\")\n",
    "\n",
    "if len(successful_df) > 0:\n",
    "    accs = successful_df['test_accuracy']\n",
    "    print(f\"\\nTest Accuracy:\")\n",
    "    print(f\"  Gemiddeld: {accs.mean():.1%}\")\n",
    "    print(f\"  Mediaan:   {accs.median():.1%}\")\n",
    "    print(f\"  Min:       {accs.min():.1%}\")\n",
    "    print(f\"  Max:       {accs.max():.1%}\")\n",
    "    print(f\"  >90%:      {(accs > 0.9).sum()} soorten\")\n",
    "    print(f\"  >80%:      {(accs > 0.8).sum()} soorten\")\n",
    "    print(f\"  <50%:      {(accs < 0.5).sum()} soorten\")\n",
    "\n",
    "    # Top 10 en Bottom 10\n",
    "    print(f\"\\n\\U0001f3c6 Top 10 (beste modellen):\")\n",
    "    for _, row in successful_df.nlargest(10, 'test_accuracy').iterrows():\n",
    "        print(f\"  {row['test_accuracy']:.1%} - {row['species']}\")\n",
    "\n",
    "    print(f\"\\n\\u26a0\\ufe0f Bottom 10 (meeste verbetering nodig):\")\n",
    "    for _, row in successful_df.nsmallest(10, 'test_accuracy').iterrows():\n",
    "        print(f\"  {row['test_accuracy']:.1%} - {row['species']}\")\n",
    "\n",
    "    # Meest verwarde soorten\n",
    "    print(f\"\\n\\U0001f500 Meest verwarde types:\")\n",
    "    confusion_scores = []\n",
    "    for species, cm in all_confusions.items():\n",
    "        total = sum(cm.values())\n",
    "        errors = sum(v for k, v in cm.items() if k.split('>')[0] != k.split('>')[1])\n",
    "        if total > 0:\n",
    "            confusion_scores.append((species, errors / total, errors, total))\n",
    "    confusion_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    for species, rate, errors, total in confusion_scores[:10]:\n",
    "        print(f\"  {rate:.0%} verward ({errors}/{total}) - {species}\")\n",
    "\n",
    "    # Histogram\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    axes[0].hist(accs, bins=20, edgecolor='black', color='steelblue')\n",
    "    axes[0].axvline(accs.mean(), color='red', linestyle='--', label=f'Gem: {accs.mean():.1%}')\n",
    "    axes[0].axvline(accs.median(), color='orange', linestyle='--', label=f'Med: {accs.median():.1%}')\n",
    "    axes[0].set_xlabel('Test Accuracy')\n",
    "    axes[0].set_ylabel('Aantal soorten')\n",
    "    axes[0].set_title('Accuracy Verdeling v2')\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Confusion rate histogram\n",
    "    if confusion_scores:\n",
    "        rates = [s[1] for s in confusion_scores]\n",
    "        axes[1].hist(rates, bins=20, edgecolor='black', color='coral')\n",
    "        axes[1].set_xlabel('Verwarrings-percentage')\n",
    "        axes[1].set_ylabel('Aantal soorten')\n",
    "        axes[1].set_title('Type Verwarring per Soort')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{DRIVE_BASE}/results_v2.png', dpi=100)\n",
    "    plt.show()\n",
    "\n",
    "# Mislukte soorten\n",
    "failed_df = df[df['status'] != 'success']\n",
    "if len(failed_df) > 0:\n",
    "    print(f\"\\n\\u274c Mislukt ({len(failed_df)}):\")\n",
    "    for status, group in failed_df.groupby('status'):\n",
    "        print(f\"  {status}: {', '.join(group['species'].tolist())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cel 12: HiDrive status & download naar Pi\n#\n# Modellen zijn al automatisch ge-upload tijdens training.\n# Deze cel toont wat er op HiDrive staat en hoe je ze naar de Pi haalt.\n#\n\nprint(f\"{'='*60}\")\nprint(f\"\\U0001f4c1 HIDRIVE MODEL STATUS\")\nprint(f\"{'='*60}\")\n\n# Toon modellen op HiDrive\nprint(f\"\\nModellen op HiDrive ({HIDRIVE_MODELS}):\")\n!rclone ls hidrive:{HIDRIVE_MODELS} 2>/dev/null | wc -l | xargs -I{} echo \"  Totaal: {} bestanden\"\n!rclone size hidrive:{HIDRIVE_MODELS} 2>/dev/null\n\nprint(f\"\\nResultaten op HiDrive ({HIDRIVE_RESULTS}):\")\n!rclone ls hidrive:{HIDRIVE_RESULTS} 2>/dev/null\n\nprint(f\"\\n{'='*60}\")\nprint(f\"DOWNLOAD NAAR PI\")\nprint(f\"{'='*60}\")\nprint(f\"\"\"\nDraai dit commando op je Pi (emsn2-zolder) om de v2 modellen op te halen:\n\n  # Maak map aan\n  mkdir -p /mnt/nas-docker/emsn-vocalization/data/models\n\n  # Download v2 modellen van HiDrive naar NAS\n  rclone copy hidrive:{HIDRIVE_MODELS}/ \\\\\n    /mnt/nas-docker/emsn-vocalization/data/models/ \\\\\n    --progress --sftp-key-file ~/.ssh/id_ed25519_hidrive\n\n  # Of configureer rclone eerst:\n  # rclone config (type=sftp, host=sftp.hidrive.strato.com,\n  #                user=ronnyclouddisk, key_file=~/.ssh/id_ed25519_hidrive)\n\n  # Herstart vocalization enricher om nieuwe modellen te laden\n  sudo systemctl restart vocalization-enricher\n\nKlaar! De v2 modellen worden automatisch gebruikt (hogere prioriteit).\n\"\"\")"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}